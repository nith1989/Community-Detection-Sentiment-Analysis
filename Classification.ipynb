{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "classify.py\n",
    "\"\"\"\n",
    "# Loading required libraries\n",
    "import re\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Read previously loaded file with tweets\n",
    "def read_file():\n",
    "    return pickle.load(open('nmodi_tweets.pkl', 'rb'))\n",
    "\n",
    "# Tokenization for tweets\n",
    "def tokenize(t):\n",
    "    # Remove mentions, URLs & convert to words\n",
    "    text = t.lower()\n",
    "    text = re.sub('@\\S+', ' ', text)  \n",
    "    text = re.sub('http\\S+', ' ', text) \n",
    "    return re.findall('[A-Za-z]+', text) \n",
    "\n",
    "# Download the AFINN lexicon\n",
    "def download_afinn():\n",
    "    from collections import defaultdict\n",
    "    from io import BytesIO\n",
    "    from zipfile import ZipFile\n",
    "    from urllib.request import urlopen\n",
    "\n",
    "    url = urlopen('http://www2.compute.dtu.dk/~faan/data/AFINN.zip')\n",
    "    zipfile = ZipFile(BytesIO(url.read()))\n",
    "    afinn_file = zipfile.open('AFINN/AFINN-111.txt')\n",
    "    afinn = dict()\n",
    "\n",
    "    for line in afinn_file:\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) == 2:\n",
    "            afinn[parts[0].decode(\"utf-8\")] = int(parts[1])\n",
    "    return afinn\n",
    "\n",
    "# Classify sentiment as positive or negative\n",
    "def afinn_sentiment2(terms, afinn, verbose=False):\n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    for t in terms:\n",
    "        if t in afinn:\n",
    "            if afinn[t] > 0:\n",
    "                pos += afinn[t]\n",
    "            else:\n",
    "                neg += -1 * afinn[t]\n",
    "    return pos, neg\n",
    "\n",
    "# Get all files from a specific path\n",
    "def get_files(path):\n",
    "    text_files = [os.path.join(path,file) for file in os.listdir(path) if file.endswith('.txt')]\n",
    "    return sorted(text_files)\n",
    "\n",
    "# Read and tokenize each document\n",
    "def read_n_tokenize(train_docs):\n",
    "    if (isinstance(train_docs, list) == False):\n",
    "        train_docs = train_docs.tolist()\n",
    "    new_list = []\n",
    "    tweets = []\n",
    "    for e in train_docs:\n",
    "        file = open(e, 'r', encoding='utf8')\n",
    "        new_tweet = str(file.read())\n",
    "        tweets.append(new_tweet)\n",
    "        new_list.append(tokenize(new_tweet))\n",
    "\n",
    "    return new_list, tweets\n",
    "\n",
    "# Classify the sentiment in these tweets\n",
    "def classify(new_list, tweets):\n",
    "    positives = []\n",
    "    negatives = []\n",
    "    new_path = 'Classified'\n",
    "    afinn=download_afinn()\n",
    "    val = 0\n",
    "    for token_list, tweet in zip(new_list, tweets):\n",
    "        pos, neg = afinn_sentiment2(token_list, afinn)\n",
    "        if pos > neg:\n",
    "            positives.append((tweet, pos, neg))\n",
    "        elif neg > pos:\n",
    "            negatives.append((tweet, pos, neg))\n",
    "    file = open(\"Classification Summary.txt\", \"w\", encoding='utf8')\n",
    "    file.write(\"Number of instances(tweets) per class found: \")\n",
    "    file.write(\"\\nPositive Class: \"+ str(len(positives)))\n",
    "    file.write(\"\\nNegative Class: \"+ str(len(negatives)))\n",
    "    file.write(\"\\n\\nOne example from each class:\")\n",
    "    file.write(\"\\nPositive Class: \\n\"+ str(sorted(positives, key=lambda x: x[1], reverse=True)[:1]))\n",
    "    file.write(\"\\nNegative Class: \\n\"+ str(sorted(negatives, key=lambda x: x[2], reverse=True)[:1]))\n",
    "    file.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    path = 'Tweets Data/'\n",
    "    train_docs = get_files(path)\n",
    "    new_list, tweets = read_n_tokenize(train_docs)\n",
    "    classify(new_list, tweets)\n",
    "\n",
    "if __name__=='__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "classify.py\n",
    "\"\"\"\n",
    "\n",
    "# Loading required libraries\n",
    "import re\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Read previously loaded file with tweets\n",
    "def read_file():\n",
    "    return pickle.load(open('nmodi_tweets.pkl', 'rb'))\n",
    "\n",
    "# Tokenization for tweets\n",
    "def tokenize(t):\n",
    "    # Remove mentions, URLs & convert to words\n",
    "    text = t.lower()\n",
    "    text = re.sub('@\\S+', ' ', text)  \n",
    "    text = re.sub('http\\S+', ' ', text) \n",
    "    return re.findall('[A-Za-z]+', text) \n",
    "\n",
    "# Download the AFINN lexicon\n",
    "def download_afinn():\n",
    "    from collections import defaultdict\n",
    "    from io import BytesIO\n",
    "    from zipfile import ZipFile\n",
    "    from urllib.request import urlopen\n",
    "\n",
    "    url = urlopen('http://www2.compute.dtu.dk/~faan/data/AFINN.zip')\n",
    "    zipfile = ZipFile(BytesIO(url.read()))\n",
    "    afinn_file = zipfile.open('AFINN/AFINN-111.txt')\n",
    "    afinn = dict()\n",
    "\n",
    "    for line in afinn_file:\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) == 2:\n",
    "            afinn[parts[0].decode(\"utf-8\")] = int(parts[1])\n",
    "    return afinn\n",
    "\n",
    "# Classify sentiment as positive or negative\n",
    "def afinn_sentiment2(terms, afinn, verbose=False):\n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    for t in terms:\n",
    "        if t in afinn:\n",
    "            if afinn[t] > 0:\n",
    "                pos += afinn[t]\n",
    "            else:\n",
    "                neg += -1 * afinn[t]\n",
    "    return pos, neg\n",
    "\n",
    "# Get all files from a specific path\n",
    "def get_files(path):\n",
    "    text_files = [os.path.join(path,file) for file in os.listdir(path) if file.endswith('.txt')]\n",
    "    return sorted(text_files)\n",
    "\n",
    "# Read and tokenize each document\n",
    "def read_n_tokenize(train_docs):\n",
    "    if (isinstance(train_docs, list) == False):\n",
    "        train_docs = train_docs.tolist()\n",
    "    new_list = []\n",
    "    tweets = []\n",
    "    for e in train_docs:\n",
    "        file = open(e, 'r', encoding='utf8')\n",
    "        new_tweet = str(file.read())\n",
    "        tweets.append(new_tweet)\n",
    "        new_list.append(tokenize(new_tweet))\n",
    "\n",
    "    return new_list, tweets\n",
    "\n",
    "# Classify the sentiment in these tweets\n",
    "def classify(new_list, tweets):\n",
    "    positives = []\n",
    "    negatives = []\n",
    "    new_path = 'Classified'\n",
    "    afinn=download_afinn()\n",
    "    val = 0\n",
    "    for token_list, tweet in zip(new_list, tweets):\n",
    "        pos, neg = afinn_sentiment2(token_list, afinn)\n",
    "        if pos > neg:\n",
    "            positives.append((tweet, pos, neg))\n",
    "        elif neg > pos:\n",
    "            negatives.append((tweet, pos, neg))\n",
    "    file = open(\"Classification Summary.txt\", \"w\", encoding='utf8')\n",
    "    file.write(\"Number of instances(tweets) per class found: \")\n",
    "    file.write(\"\\nPositive Class: \"+ str(len(positives)))\n",
    "    file.write(\"\\nNegative Class: \"+ str(len(negatives)))\n",
    "    file.write(\"\\n\\nOne example from each class:\")\n",
    "    file.write(\"\\nPositive Class: \\n\"+ str(sorted(positives, key=lambda x: x[1], reverse=True)[:1]))\n",
    "    file.write(\"\\nNegative Class: \\n\"+ str(sorted(negatives, key=lambda x: x[2], reverse=True)[:1]))\n",
    "    file.close\n",
    "\n",
    "def main():\n",
    "    path = 'Tweets Data/'\n",
    "    train_docs = get_files(path)\n",
    "    new_list, tweets = read_n_tokenize(train_docs)\n",
    "    classify(new_list, tweets)\n",
    "\n",
    "if __name__=='__main__':\n",
    "    main()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
